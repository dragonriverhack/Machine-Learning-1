{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读存数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark import SparkConf,SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My APP\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "# 1.读取文本文件\n",
    "lines = sc.textFile('file:///Users/fire/jupyter/data/ball2018.txt')\n",
    "# 存储文本\n",
    "#lines.saveAsTextFile('file:///Users/fire/jupyter/data/ball20182.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntweets = hiveCtx.jsonFile(\\'xx.json\\')\\ntweets.registerTempTable(\"xx\")\\nresults = hiveCtx.sql(\"SELECT user.name,text FROM xx\")\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.读取json\n",
    "import json\n",
    "data = Input.map(lambda x: json.loads(x))\n",
    "# 存储json\n",
    "outputFilePath = 'file:///Users/fire/jupyter/data/ball2018.txt'\n",
    "#(data.filter(lambda x:x[\"lovesPandas\"]).map(lambda x:json.dumps(x)).saveAsTextFile(outputFilePath))\n",
    "\n",
    "#结构化json\n",
    "'''\n",
    "tweets = hiveCtx.jsonFile('xx.json')\n",
    "tweets.registerTempTable(\"xx\")\n",
    "results = hiveCtx.sql(\"SELECT user.name,text FROM xx\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pandaLovers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a20e0b955dfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mpandaLovers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwritRecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pandaLovers' is not defined"
     ]
    }
   ],
   "source": [
    "# 3.读取csv\n",
    "import csv\n",
    "import io\n",
    "\n",
    "\n",
    "inputFile = \"file:///Users/fire/jupyter/data/browse.csv\"\n",
    "\n",
    "def loadRecord(line):\n",
    "    #解析一行CSV记录\n",
    "    Input = io.StringIO(line)\n",
    "    reader = csv.DictReader(Input, fieldnames=[\"name\",\"favouriteAnimal\"])\n",
    "    return reader.next()\n",
    "Input = sc.textFile(inputFile).map(loadRecord)\n",
    "\n",
    "\n",
    "# 完整读取CSV\n",
    "def loadRecords(fileNameContents):\n",
    "    #读取所有记录\n",
    "    Input = io.StringIO(fileNameContents[1])\n",
    "    reader = csv.DictReader(Input,fieldnames = [\"name\",\"favoriteAnimal\"])\n",
    "    return reader\n",
    "\n",
    "fullFileData = sc.wholeTextFiles(inputFile).flatMap(loadRecords)\n",
    "\n",
    "# 保存CSV\n",
    "def writeRecords(records):\n",
    "    output = io.StringIO()\n",
    "    writer = csv.DictWriter(output,fieldnames=[\"name\",\"favoriteAnimal\"])\n",
    "    for record in records:\n",
    "        writer.writerow(record)\n",
    "    return [output.getvalue()]\n",
    "\n",
    "pandaLovers.mapPartitions(writRecords).saveAsTextFile(outputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inFile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-445409de308f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 4. SequenceFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequenceFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minFile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"org.apache.hadoop.io.Text\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"org.apache.hadoop.io.IntWritable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'inFile' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. SequenceFile \n",
    "\n",
    "data = sc.sequenceFile(inFile,\"org.apache.hadoop.io.Text\",\"org.apache.hadoop.io.IntWritable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhiveCtx = HiveContext(sc)\\nrows = hiveCtx.sql(\"SELECT name, age FROM users\")\\nfirstRow = rows.first()\\nprint(firstRow.name)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Hive\n",
    "#创建HiveContext并查询数据\n",
    "from pyspark.sql import HiveContext\n",
    "'''\n",
    "hiveCtx = HiveContext(sc)\n",
    "rows = hiveCtx.sql(\"SELECT name, age FROM users\")\n",
    "firstRow = rows.first()\n",
    "print(firstRow.name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
