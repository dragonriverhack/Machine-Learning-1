{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 作者：Fire\n",
    "\n",
    "> 截止日期：2018/6/15\n",
    "\n",
    "> 截止期数：18068\n",
    "\n",
    "使用了从03年到截止期数的所有数据，但是也才两千多条，所以结果可能不会太好。肯定会过拟合\n",
    "\n",
    "### Future work\n",
    "* 玄学调参\n",
    "* 构造数据集，每期相差一个红球也可以，这样数据集成6的指数级增长。但是只能用作训练不同模型，因为时间序列是固定的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.core import Dense, RepeatVector\n",
    "from keras.layers import Dropout\n",
    "\n",
    "#玄学参数 随机种子\n",
    "win_number = 2018\n",
    "from numpy.random import seed\n",
    "seed(win_number)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(win_number)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2278, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red1</th>\n",
       "      <th>red2</th>\n",
       "      <th>red3</th>\n",
       "      <th>red4</th>\n",
       "      <th>red5</th>\n",
       "      <th>red6</th>\n",
       "      <th>blue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   red1  red2  red3  red4  red5  red6  blue\n",
       "0    10    11    12    13    26    28    11\n",
       "1     4     9    19    20    21    26    12\n",
       "2     1     7    10    23    28    32    16\n",
       "3     4     6     7    10    13    25     3\n",
       "4     4     6    15    17    30    31    16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.read_csv('./data/ballHisAll.csv')\n",
    "print(history.shape)\n",
    "history.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.红球seq2seq\n",
    "\n",
    "* 思路：每次通过上一期和当前期建立一个序列对，变成一个长度固定为6的标准seq2seq问题。\n",
    "* 假设：\n",
    "    1. 彩票客观/主管上都有非随机因素影响（目前人类也无法实现真正的随机），那么假设彩票每次出的号码目标是让中奖人数最少，从而最大化利益，那么就是通过上个序列预测概率最小的下个序列；\n",
    "    2. 全国彩民买彩票都会受前几期影响，主要是前一期，这里为了简化也只考虑前一期，然后分析这一期会出的号码，我们的模型就是反向预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2277, 6, 33)\n",
      "(2277, 6, 33)\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "hislen = history.shape[0]\n",
    "\n",
    "history0 = history-1 #从0计数\n",
    "dataX = np.array(history0.iloc[:hislen-1,:6])\n",
    "x_oh = []\n",
    "for i in dataX:\n",
    "    x_oh.append(keras.utils.np_utils.to_categorical(i, 33))\n",
    "x_oh = np.array(x_oh)\n",
    "print(x_oh.shape)\n",
    "\n",
    "dataY = np.array(history0.iloc[1:hislen,:6])\n",
    "y_oh = []\n",
    "for i in dataY:\n",
    "    y_oh.append(keras.utils.np_utils.to_categorical(i, 33))\n",
    "\n",
    "y_oh = np.array(y_oh)\n",
    "print(y_oh.shape)\n",
    "#X = np.reshape(dataX, (len(dataX), 7, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fire/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/fire/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=False, input_shape=(None, 33), units=256)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/fire/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"linear\", units=33)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def build_model(input_size, max_out_seq_len, hidden_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(input_dim=input_size, output_dim=hidden_size, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(hidden_size, activation=\"relu\"))\n",
    "    model.add(RepeatVector(max_out_seq_len))\n",
    "    model.add(LSTM(hidden_size, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(output_dim=input_size, activation=\"linear\")))\n",
    "    model.compile(loss=\"mse\", optimizer='adam')\n",
    "    #print(model.summary())\n",
    "    return model\n",
    "\n",
    "model = build_model(33,6,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2049 samples, validate on 228 samples\n",
      "Epoch 1/120\n",
      "2049/2049 [==============================] - 4s - loss: 0.0292 - val_loss: 0.0289\n",
      "Epoch 2/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0286 - val_loss: 0.0288\n",
      "Epoch 3/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0284 - val_loss: 0.0288\n",
      "Epoch 4/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0283 - val_loss: 0.0288\n",
      "Epoch 5/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0282 - val_loss: 0.0287\n",
      "Epoch 6/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0282 - val_loss: 0.0287\n",
      "Epoch 7/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0281 - val_loss: 0.0288\n",
      "Epoch 8/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0281 - val_loss: 0.0287\n",
      "Epoch 9/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0280 - val_loss: 0.0287\n",
      "Epoch 10/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0280 - val_loss: 0.0288\n",
      "Epoch 11/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0279 - val_loss: 0.0289\n",
      "Epoch 12/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0279 - val_loss: 0.0288\n",
      "Epoch 13/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0278 - val_loss: 0.0289\n",
      "Epoch 14/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0277 - val_loss: 0.0289\n",
      "Epoch 15/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0277 - val_loss: 0.0290\n",
      "Epoch 16/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0276 - val_loss: 0.0290\n",
      "Epoch 17/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0276 - val_loss: 0.0291\n",
      "Epoch 18/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0276 - val_loss: 0.0291\n",
      "Epoch 19/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0275 - val_loss: 0.0292\n",
      "Epoch 20/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0275 - val_loss: 0.0294\n",
      "Epoch 21/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0274 - val_loss: 0.0292\n",
      "Epoch 22/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0274 - val_loss: 0.0296\n",
      "Epoch 23/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0274 - val_loss: 0.0293\n",
      "Epoch 24/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0273 - val_loss: 0.0293\n",
      "Epoch 25/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0272 - val_loss: 0.0297\n",
      "Epoch 26/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0273 - val_loss: 0.0294\n",
      "Epoch 27/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0271 - val_loss: 0.0295\n",
      "Epoch 28/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0270 - val_loss: 0.0294\n",
      "Epoch 29/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0269 - val_loss: 0.0295\n",
      "Epoch 30/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0268 - val_loss: 0.0297\n",
      "Epoch 31/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0268 - val_loss: 0.0297\n",
      "Epoch 32/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0267 - val_loss: 0.0299\n",
      "Epoch 33/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0268 - val_loss: 0.0299\n",
      "Epoch 34/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0269 - val_loss: 0.0295\n",
      "Epoch 35/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0265 - val_loss: 0.0299\n",
      "Epoch 36/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0266 - val_loss: 0.0298\n",
      "Epoch 37/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0267 - val_loss: 0.0295\n",
      "Epoch 38/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0263 - val_loss: 0.0299\n",
      "Epoch 39/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0262 - val_loss: 0.0300\n",
      "Epoch 40/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0260 - val_loss: 0.0304\n",
      "Epoch 41/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0260 - val_loss: 0.0302\n",
      "Epoch 42/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0259 - val_loss: 0.0306\n",
      "Epoch 43/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0260 - val_loss: 0.0308\n",
      "Epoch 44/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0259 - val_loss: 0.0300\n",
      "Epoch 45/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0257 - val_loss: 0.0304\n",
      "Epoch 46/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0257 - val_loss: 0.0308\n",
      "Epoch 47/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0259 - val_loss: 0.0303\n",
      "Epoch 48/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0256 - val_loss: 0.0308\n",
      "Epoch 49/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0254 - val_loss: 0.0308\n",
      "Epoch 50/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0254 - val_loss: 0.0306\n",
      "Epoch 51/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0255 - val_loss: 0.0309\n",
      "Epoch 52/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0255 - val_loss: 0.0308\n",
      "Epoch 53/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0252 - val_loss: 0.0309\n",
      "Epoch 54/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0251 - val_loss: 0.0314\n",
      "Epoch 55/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0252 - val_loss: 0.0312\n",
      "Epoch 56/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0250 - val_loss: 0.0313\n",
      "Epoch 57/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0248 - val_loss: 0.0313\n",
      "Epoch 58/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0246 - val_loss: 0.0320\n",
      "Epoch 59/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0249 - val_loss: 0.0317\n",
      "Epoch 60/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0247 - val_loss: 0.0314\n",
      "Epoch 61/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0244 - val_loss: 0.0320\n",
      "Epoch 62/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0245 - val_loss: 0.0316\n",
      "Epoch 63/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0244 - val_loss: 0.0319\n",
      "Epoch 64/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0244 - val_loss: 0.0315\n",
      "Epoch 65/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0241 - val_loss: 0.0319\n",
      "Epoch 66/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0241 - val_loss: 0.0320\n",
      "Epoch 67/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0238 - val_loss: 0.0319\n",
      "Epoch 68/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0238 - val_loss: 0.0323\n",
      "Epoch 69/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0240 - val_loss: 0.0320\n",
      "Epoch 70/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0237 - val_loss: 0.0320\n",
      "Epoch 71/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0236 - val_loss: 0.0323\n",
      "Epoch 72/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0232 - val_loss: 0.0323\n",
      "Epoch 73/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0231 - val_loss: 0.0326\n",
      "Epoch 74/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0230 - val_loss: 0.0327\n",
      "Epoch 75/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0229 - val_loss: 0.0328\n",
      "Epoch 76/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0228 - val_loss: 0.0329\n",
      "Epoch 77/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0225 - val_loss: 0.0329\n",
      "Epoch 78/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0227 - val_loss: 0.0330\n",
      "Epoch 79/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0230 - val_loss: 0.0329\n",
      "Epoch 80/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0224 - val_loss: 0.0335ss: \n",
      "Epoch 81/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0227 - val_loss: 0.0331\n",
      "Epoch 82/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0233 - val_loss: 0.0327\n",
      "Epoch 83/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0225 - val_loss: 0.0333\n",
      "Epoch 84/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0227 - val_loss: 0.0331\n",
      "Epoch 85/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0224 - val_loss: 0.0336\n",
      "Epoch 86/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2049/2049 [==============================] - 3s - loss: 0.0229 - val_loss: 0.0332\n",
      "Epoch 87/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0226 - val_loss: 0.0337\n",
      "Epoch 88/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0229 - val_loss: 0.0332\n",
      "Epoch 89/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0222 - val_loss: 0.0334\n",
      "Epoch 90/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0218 - val_loss: 0.0335\n",
      "Epoch 91/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0221 - val_loss: 0.0336\n",
      "Epoch 92/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0218 - val_loss: 0.0338\n",
      "Epoch 93/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0215 - val_loss: 0.0340\n",
      "Epoch 94/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0216 - val_loss: 0.0336\n",
      "Epoch 95/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0213 - val_loss: 0.0343\n",
      "Epoch 96/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0214 - val_loss: 0.0339\n",
      "Epoch 97/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0212 - val_loss: 0.0338\n",
      "Epoch 98/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0209 - val_loss: 0.0343\n",
      "Epoch 99/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0220 - val_loss: 0.0339\n",
      "Epoch 100/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0211 - val_loss: 0.0343\n",
      "Epoch 101/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0208 - val_loss: 0.0343\n",
      "Epoch 102/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0211 - val_loss: 0.0341\n",
      "Epoch 103/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0205 - val_loss: 0.0345ss: 0 - ETA: 1s\n",
      "Epoch 104/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0205 - val_loss: 0.0342\n",
      "Epoch 105/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0204 - val_loss: 0.0343\n",
      "Epoch 106/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0202 - val_loss: 0.0343\n",
      "Epoch 107/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0202 - val_loss: 0.0347\n",
      "Epoch 108/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0198 - val_loss: 0.0346\n",
      "Epoch 109/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0195 - val_loss: 0.0348\n",
      "Epoch 110/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0200 - val_loss: 0.0346\n",
      "Epoch 111/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0196 - val_loss: 0.0347\n",
      "Epoch 112/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0194 - val_loss: 0.0351\n",
      "Epoch 113/120\n",
      "2049/2049 [==============================] - 3s - loss: 0.0194 - val_loss: 0.0349\n",
      "Epoch 114/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0191 - val_loss: 0.0354\n",
      "Epoch 115/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0189 - val_loss: 0.0354\n",
      "Epoch 116/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0190 - val_loss: 0.0352\n",
      "Epoch 117/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0188 - val_loss: 0.0351\n",
      "Epoch 118/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0188 - val_loss: 0.0353\n",
      "Epoch 119/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0187 - val_loss: 0.0358\n",
      "Epoch 120/120\n",
      "2049/2049 [==============================] - 2s - loss: 0.0188 - val_loss: 0.0353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd362b5f60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_oh,y_oh,\n",
    "          batch_size=64,\n",
    "          epochs=120,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last red ball:  [11, 16, 19, 22, 25, 30]\n",
      "Next red ball:  [3, 4, 19, 22, 29, 32]  \n",
      "是否修改过：  否\n"
     ]
    }
   ],
   "source": [
    "test_x = np.reshape(y_oh[-1], (1, 6, 33))\n",
    "print('Last red ball: ',[np.argmax(x)+1 for x in test_x[0]])\n",
    "test_y = model.predict(test_x)\n",
    "final_red = [np.argmax(x)+1 for x in test_y[-1]]\n",
    "#解决重号的问题 \n",
    "count33 = 2\n",
    "inWhile = 0\n",
    "while len(set(final_red)) <6:\n",
    "    inWhile = 1\n",
    "    for i in range(1,6):\n",
    "        if final_red[i] == final_red[i-1]:\n",
    "            final_red[i]+=1\n",
    "            if final_red[i]>33:\n",
    "                final_red[i] -= count33\n",
    "                final_red = sorted(final_red)\n",
    "                count33+=1\n",
    "print('Next red ball: ',final_red ,' \\n是否修改过： ', '是' if inWhile else '否')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.蓝球lstm 多到1\n",
    "* 思路：蓝球单独处理，利用前N期（比如10）期号码做成一个时间序列，预测下一期的号码。即多到一的预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2258, 20, 16)\n",
      "(2258, 16)\n"
     ]
    }
   ],
   "source": [
    "blueAll = np.array(history0.iloc[:,6:7])\n",
    "seq_N = 20\n",
    "blue_train = []\n",
    "blue_label = []\n",
    "for i in range(seq_N,len(blueAll)):\n",
    "    x_tmp = blueAll[i-seq_N:i][::-1] ## 小trick。逆序\n",
    "    x_tmp = keras.utils.np_utils.to_categorical(x_tmp, 16)\n",
    "    blue_train.append(x_tmp)\n",
    "    \n",
    "    y_tmp = blueAll[i]\n",
    "    y_tmp = keras.utils.np_utils.to_categorical(y_tmp, 16)\n",
    "    blue_label.append(y_tmp)\n",
    "\n",
    "blue_train = np.array(blue_train)\n",
    "blue_label = np.array(blue_label)\n",
    "blue_label = np.reshape(blue_label,(blue_label.shape[0],blue_label.shape[2]))\n",
    "print(blue_train.shape)\n",
    "print(blue_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model2(input_size, seq_len, hidden_siz):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_siz, input_shape=( seq_len,input_size)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(input_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    #print(model.summary())\n",
    "    return model\n",
    "\n",
    "model2 = build_model2(16,seq_N,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2032 samples, validate on 226 samples\n",
      "Epoch 1/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.7737 - val_loss: 2.7717\n",
      "Epoch 2/120\n",
      "2032/2032 [==============================] - 1s - loss: 2.7656 - val_loss: 2.7733\n",
      "Epoch 3/120\n",
      "2032/2032 [==============================] - 1s - loss: 2.7591 - val_loss: 2.7707\n",
      "Epoch 4/120\n",
      "2032/2032 [==============================] - 1s - loss: 2.7531 - val_loss: 2.7804\n",
      "Epoch 5/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.7436 - val_loss: 2.7825\n",
      "Epoch 6/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.7333 - val_loss: 2.7775\n",
      "Epoch 7/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.7207 - val_loss: 2.8013\n",
      "Epoch 8/120\n",
      "2032/2032 [==============================] - 1s - loss: 2.7105 - val_loss: 2.8034\n",
      "Epoch 9/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.6932 - val_loss: 2.8579\n",
      "Epoch 10/120\n",
      "2032/2032 [==============================] - 1s - loss: 2.6809 - val_loss: 2.8798\n",
      "Epoch 11/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.6588 - val_loss: 2.8462\n",
      "Epoch 12/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.6416 - val_loss: 2.8558\n",
      "Epoch 13/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.6098 - val_loss: 2.8895\n",
      "Epoch 14/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.5923 - val_loss: 2.9163\n",
      "Epoch 15/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.5595 - val_loss: 2.9348\n",
      "Epoch 16/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.5383 - val_loss: 2.9683\n",
      "Epoch 17/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.4948 - val_loss: 3.0291\n",
      "Epoch 18/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.4621 - val_loss: 3.1016\n",
      "Epoch 19/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.4109 - val_loss: 3.1369\n",
      "Epoch 20/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.3923 - val_loss: 3.1240\n",
      "Epoch 21/120\n",
      "2032/2032 [==============================] - 3s - loss: 2.3425 - val_loss: 3.3031\n",
      "Epoch 22/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.3109 - val_loss: 3.2930\n",
      "Epoch 23/120\n",
      "2032/2032 [==============================] - 1s - loss: 2.2442 - val_loss: 3.3660\n",
      "Epoch 24/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.2183 - val_loss: 3.3182\n",
      "Epoch 25/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.1559 - val_loss: 3.3688\n",
      "Epoch 26/120\n",
      "2032/2032 [==============================] - 1s - loss: 2.1021 - val_loss: 3.5515\n",
      "Epoch 27/120\n",
      "2032/2032 [==============================] - 2s - loss: 2.0407 - val_loss: 3.5584\n",
      "Epoch 28/120\n",
      "2032/2032 [==============================] - 2s - loss: 1.9474 - val_loss: 3.5726\n",
      "Epoch 29/120\n",
      "2032/2032 [==============================] - 2s - loss: 1.8754 - val_loss: 3.6750\n",
      "Epoch 30/120\n",
      "2032/2032 [==============================] - 1s - loss: 1.8143 - val_loss: 3.8048\n",
      "Epoch 31/120\n",
      "2032/2032 [==============================] - 2s - loss: 1.7635 - val_loss: 3.8138\n",
      "Epoch 32/120\n",
      "2032/2032 [==============================] - 2s - loss: 1.6959 - val_loss: 4.0014\n",
      "Epoch 33/120\n",
      "2032/2032 [==============================] - 1s - loss: 1.6104 - val_loss: 3.9252\n",
      "Epoch 34/120\n",
      "2032/2032 [==============================] - 2s - loss: 1.5494 - val_loss: 4.0239\n",
      "Epoch 35/120\n",
      "2032/2032 [==============================] - 2s - loss: 1.4624 - val_loss: 4.1669\n",
      "Epoch 36/120\n",
      "2032/2032 [==============================] - 2s - loss: 1.4043 - val_loss: 4.2248\n",
      "Epoch 37/120\n",
      "2032/2032 [==============================] - 2s - loss: 1.3274 - val_loss: 4.2340\n",
      "Epoch 38/120\n",
      "2032/2032 [==============================] - 2s - loss: 1.2611 - val_loss: 4.3256\n",
      "Epoch 39/120\n",
      "2032/2032 [==============================] - 1s - loss: 1.1727 - val_loss: 4.4652\n",
      "Epoch 40/120\n",
      "2032/2032 [==============================] - 2s - loss: 1.0928 - val_loss: 4.4130\n",
      "Epoch 41/120\n",
      "2032/2032 [==============================] - 2s - loss: 1.0126 - val_loss: 4.6229\n",
      "Epoch 42/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.9279 - val_loss: 4.7963\n",
      "Epoch 43/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.8960 - val_loss: 4.8644\n",
      "Epoch 44/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.8477 - val_loss: 4.9887\n",
      "Epoch 45/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.7945 - val_loss: 5.0356\n",
      "Epoch 46/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.7413 - val_loss: 5.0805\n",
      "Epoch 47/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.6809 - val_loss: 5.2760\n",
      "Epoch 48/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.6130 - val_loss: 5.3793\n",
      "Epoch 49/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.5738 - val_loss: 5.4468\n",
      "Epoch 50/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.5428 - val_loss: 5.4527\n",
      "Epoch 51/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.5324 - val_loss: 5.5449\n",
      "Epoch 52/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.4672 - val_loss: 5.6237\n",
      "Epoch 53/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.4114 - val_loss: 5.7202\n",
      "Epoch 54/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.3740 - val_loss: 5.8414\n",
      "Epoch 55/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.3674 - val_loss: 5.6437\n",
      "Epoch 56/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.3511 - val_loss: 6.0619\n",
      "Epoch 57/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.3120 - val_loss: 6.1004\n",
      "Epoch 58/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.2962 - val_loss: 6.0377\n",
      "Epoch 59/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.2951 - val_loss: 6.0752\n",
      "Epoch 60/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.2610 - val_loss: 6.0788\n",
      "Epoch 61/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.2579 - val_loss: 6.3212\n",
      "Epoch 62/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.2623 - val_loss: 6.3776\n",
      "Epoch 63/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.2626 - val_loss: 6.2121\n",
      "Epoch 64/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.2231 - val_loss: 6.4042\n",
      "Epoch 65/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.1776 - val_loss: 6.4967\n",
      "Epoch 66/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.1490 - val_loss: 6.7636\n",
      "Epoch 67/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.1240 - val_loss: 6.7460\n",
      "Epoch 68/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0991 - val_loss: 6.8023\n",
      "Epoch 69/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0836 - val_loss: 6.8028\n",
      "Epoch 70/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0739 - val_loss: 6.9741\n",
      "Epoch 71/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0671 - val_loss: 7.0053\n",
      "Epoch 72/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0586 - val_loss: 7.0528\n",
      "Epoch 73/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0560 - val_loss: 7.1075\n",
      "Epoch 74/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0508 - val_loss: 7.1064\n",
      "Epoch 75/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0496 - val_loss: 7.1929\n",
      "Epoch 76/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0495 - val_loss: 7.2827\n",
      "Epoch 77/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0530 - val_loss: 7.2889\n",
      "Epoch 78/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0767 - val_loss: 7.1535\n",
      "Epoch 79/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0928 - val_loss: 7.2288\n",
      "Epoch 80/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.2107 - val_loss: 7.1917\n",
      "Epoch 81/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.4344 - val_loss: 7.0066\n",
      "Epoch 82/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.5599 - val_loss: 6.7700\n",
      "Epoch 83/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.4500 - val_loss: 6.7342\n",
      "Epoch 84/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.3483 - val_loss: 6.7954\n",
      "Epoch 85/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.2491 - val_loss: 6.8862\n",
      "Epoch 86/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2032/2032 [==============================] - 1s - loss: 0.1755 - val_loss: 7.1755\n",
      "Epoch 87/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.1165 - val_loss: 7.1116\n",
      "Epoch 88/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.1041 - val_loss: 7.2364\n",
      "Epoch 89/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0670 - val_loss: 7.4001\n",
      "Epoch 90/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0434 - val_loss: 7.3801\n",
      "Epoch 91/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0339 - val_loss: 7.4696\n",
      "Epoch 92/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0292 - val_loss: 7.5367\n",
      "Epoch 93/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0276 - val_loss: 7.6039\n",
      "Epoch 94/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0237 - val_loss: 7.6481\n",
      "Epoch 95/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0252 - val_loss: 7.6771\n",
      "Epoch 96/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0213 - val_loss: 7.7003\n",
      "Epoch 97/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0206 - val_loss: 7.7071\n",
      "Epoch 98/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0200 - val_loss: 7.7457\n",
      "Epoch 99/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0197 - val_loss: 7.7364\n",
      "Epoch 100/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0180 - val_loss: 7.7805\n",
      "Epoch 101/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0168 - val_loss: 7.7891\n",
      "Epoch 102/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0171 - val_loss: 7.8015\n",
      "Epoch 103/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0163 - val_loss: 7.8390\n",
      "Epoch 104/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0153 - val_loss: 7.8812\n",
      "Epoch 105/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0155 - val_loss: 7.8822\n",
      "Epoch 106/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0148 - val_loss: 7.9497\n",
      "Epoch 107/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.0144 - val_loss: 7.9437\n",
      "Epoch 108/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.0164 - val_loss: 8.0200\n",
      "Epoch 109/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.0176 - val_loss: 7.9834\n",
      "Epoch 110/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0140 - val_loss: 8.0836\n",
      "Epoch 111/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0143 - val_loss: 8.0617\n",
      "Epoch 112/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0133 - val_loss: 8.0710\n",
      "Epoch 113/120\n",
      "2032/2032 [==============================] - 1s - loss: 0.0114 - val_loss: 8.1166\n",
      "Epoch 114/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.0106 - val_loss: 8.0915\n",
      "Epoch 115/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.0105 - val_loss: 8.1505\n",
      "Epoch 116/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.0098 - val_loss: 8.1656\n",
      "Epoch 117/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.0099 - val_loss: 8.1982\n",
      "Epoch 118/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.0097 - val_loss: 8.2139\n",
      "Epoch 119/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.0102 - val_loss: 8.2295\n",
      "Epoch 120/120\n",
      "2032/2032 [==============================] - 2s - loss: 0.0094 - val_loss: 8.2175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd38376eb8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(blue_train,blue_label,\n",
    "          batch_size=64,\n",
    "          epochs=120,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last blue ball:  [6, 16, 7, 1, 13, 7, 11, 10, 7, 11, 12, 5, 1, 10, 5, 1, 12, 10, 3, 8]\n",
      "Next blue ball:  1\n",
      "Final all ball:  [3, 4, 19, 22, 29, 32] 1\n"
     ]
    }
   ],
   "source": [
    "true_last =  keras.utils.np_utils.to_categorical(blueAll[-seq_N:], 16)\n",
    "test_x1 = np.reshape(true_last, (1, 20, 16))\n",
    "print('Last blue ball: ',[np.argmax(x)+1 for x in test_x1[0]])\n",
    "test_y1 = model2.predict(test_x1)\n",
    "final_blue = np.argmax(test_y1)\n",
    "print('Next blue ball: ',final_blue+1)\n",
    "print('Final all ball: ',final_red,final_blue+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
