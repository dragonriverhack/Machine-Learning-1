# 《深度卷积网络：原理与实践》笔记
> Fire 2019.01.12

### 前言
1. 本书选择MXNet框架的原因：训练速度快、占用资源少、使用方便、架构清晰、易于二次开发；

### 第1章 走进深度学习的世界
2. 预测学习（Predictive Learning）：输入1张图像，预测图像后续的发展；

### 第2章 深度卷机网络：第一课
3. 为何深度神经网络拥有如此强大的威力？这仍然是学术界的研究课题。目前大致的认知是，深度神经网络的逐层结构可以实现对于概念的不断抽象，这恰好与世界的运行规律吻合。
4. 经验法则：如果一项工作中所需要思考和决策的问题，人能在5秒内解决，它就很有可能被目前的深度神经网络实现。 
5. 2011年，谷歌用传统浅层神经网络用了16000台机器，计算3天，才构建出一个足以识别猫的网络；在2012年，著名的深度神经网络AlexNet面世，1台机器就可以完成这个任务；
6. 图像分割（segmentation），能进一步将图像自动划分为各个物体，并标记处每个物体的具体区域。目前主流是Mask R-CNN网络，还可进一步实现包括人体姿态识别的图像分割；
7. Tensorflow游乐场：playground.tensorflow.org
8. MNIST识别经典模型是1998年的LeNet-5网络，是DCNN的雏形，在正常情况下可达到99.05%的识别率。可访问scs.ryerson.ca/~aharley/vis/conv/，看到该网络每一层输出的图像；
9. 策略网络实例（围棋）：withablink.coding.me/goPolicyNet/


### 第3章 深度卷积网络：第二课
10. Excel实现神经网络：2.3.5-P297
11. 如果发现网络的训练性能很差，值得做的事情就是观察网络内部梯度的流动情况；改善的技巧：BN、残差网络（ResNet），梯度截断，梯度惩罚；
12. 从几何观点理解神经网络：colah.github.io/posts/2014-03-NN-Manifolds-Topology/;
13. 根据拓扑学定理，所有n维流形都可以在2n+2维空间中划分开。神经网络在隐层使用大量神经元，就是在做升维，以便划分样本，这成为disentangling，即将纠缠在一起的特征或概念分开；
14. 很多时候我们还是会根据测试集调参，因此，很多研究中已不使用验证集；
15. 半监督学习（simi-supervised learning），即数据中只有部分样本带有标签，然后希望给所有样本和未来的样本找到标签；一种有趣的方法是，先人工标记少量标签，然后从少量标签训练网络，然后让网络预测所有样本的标签，再人工筛选和修改其中的标签，重复这个过程。由于网络的预测会越来越准，因此可节省许多人工标注的时间；
16. 根据近年的研究，比如《The Loss Surfaces of Multilayer Networks》，对于大规模的神经网络，这实际影响不大。如果神经网络的规模够大够深，使用足够多的神经元，往往最后会得到相当靠近全局最优值的解；
17. 根据经验，如果数据集很复杂，那么普通的SGD虽然速度更慢，但有可能会得到更好的准确率。
18. L2和L1正则化的基本思想，最简单的网络没有连接，因此希望网络的连接越少越好，如果连接的权重为0就相当于没有连接，因此希望网络中连接的权重越小越好；
19. 两种类似dropout思想的正则化方法：随机深度和Shake-Shake正则化；
20. 多分类设置目标类别为100%不好，因为softmax的特性导致网络权重越来越大（需要输入无穷大才能输出100%），不利于网络稳定性，因此可尝试设置为95%；
21. 进一步的预处理包括白化，常用方法包括PCA和ZCA白化。对于图像还可以进行直方图均衡；
22. batch大小经验：对于常见问题，最优的往往在16-256之间，太小训练过慢，太大则性能不佳；facebook2017年论文《Accurate,Large Minibatch SGD:Traing ImageNet in 1Hour》指出，设置合理的学习率，在批大小很大（比如8192）的时候也能取得较好的性能；
23. 可通过CPU-Z和GPU-Z软件，观察CPU和GPU是否在满负荷工作；
24. 2017年提出Fashion-MNIST，比MNIST难度更大更有代表性；


### 第4章 深度卷积网络：第三课
25. 卷积操作后，图像中的值往往会有正有负，正表示与特征匹配，负表示相反。如果再进行ReLU操作，就会只留下正值，因此ReLU很适合CNN；
26. 使用奇数卷积核的好处，可以通过设置合适的padding使得图像再卷积后大小不变；
27. 转置卷积4.3.5；

### 第5章 深度卷积网络：第四课
28. 5.1.1 AlexNet的特点总结；
29. 5.1.2 VGG的特点；
30. 5.1.3 DarkNet的特点；
31. 2017年9月发布的SmoothGrad技术，找到图像特征关键区域；
32. 1*1卷积核的应用场景 5.4.2；
33. batch normalization 5.4.3；
34. 残差网络：ResNet的思想 5.5.1 、 5.5.2 残差网络架构细节；
35. 5.6.1 残差网络进展：ResNet、Pyramid Net、 DenseNet；
36. 压缩网络：SqueezeNet、MobileNet、ShuffleNet（可在AlexNet的二十分之一的运算量下实现相近性能）；
37. 5.6.3 卷积核的变形：扩张卷积（dilated convolution）、可变形卷积（deformable convolution）；
38. 5.7.1 yolo v1；5.7.3 Faster R-CNN；5.7.4 Mask-RCNN
39. 5.8 图像风格迁移；

### 第6章 AlphaGo架构综述
* 6.1.1 v13和v18；
* 6.2 对弈过程；
* 6.2.3 蒙特卡洛树搜索与估值问题；

### 第7章 训练策略网络与实战
* zero.sjeng.org

### 第8章 生成式对抗网络：GAN
* github.com/hindupuravinash/the-gan-zonn 列举了上百种不同的GAN设计；
* 8.5.1 自编码器：从AE到VAE；
* 8.5.2 逐点生成：PixelRNN和PixelCNN系列；

### 第9章 通向智能之秘
* 9.3.3 目前研究人员认为，NLP中的问题，从易到难，顺序如下：
	1. 文本搜索
	2. 文本分类、情感分析
	3. 翻译（到这里为止AI已经接近人类）
	4. 文本摘要
	5. 垂直领域问答
	6. 泛领域问答
* 9.4 深度学习理论发展、前沿研究
* 9.4.2 超越神经网络：Capsule与gcForest
* 9.4.3 深度学习为什么泛化能力好
* 研究人员发现，网络越大，越难通过训练到达全局极小值点，但是同时发现，网络越大，局部极小值点和全局极小值点的差距会越小；因此，网络越大，训练的过程会越简单，越稳定，因为到达任意一个局部极小值点酒足够了；