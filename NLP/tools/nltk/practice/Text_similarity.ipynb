{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用元素频率表示文本特征，从而表示成等长的词向量。\n",
    "\n",
    "余弦定理计算两个向量相似性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'my', 'sentence', 'this', 'is', 'my', 'life', 'this', 'is', 'the', 'day']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "# 做个词库先\n",
    "corpus = 'this is my sentence ' \\\n",
    "            'this is my life ' \\\n",
    "            'this is the day'\n",
    "# 随便便tokenize一下\n",
    "# 显然, 正如上文提到,这里可以根据需要做任何的preprocessing:stopwords, lemma, stemming, etc.\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "print(tokens)\n",
    "# 得到token好的word list\n",
    "\n",
    "# 借用NLTK的FreqDist统计一下文字出现的频率\n",
    "fdist = FreqDist(tokens)\n",
    "# 它就类似于一个Dict\n",
    "# 带上某个单词, 可以看到它在整个⽂文章中出现的次数\n",
    "print(fdist['is'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 3), ('this', 3), ('my', 2), ('sentence', 1), ('the', 1), ('life', 1), ('day', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 好, 此刻, 我们可以把最常用的50个单词拿出来\n",
    "standard_freq_vector = fdist.most_common(50)\n",
    "size = len(standard_freq_vector)\n",
    "print(standard_freq_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 2, 'is': 0, 'sentence': 3, 'the': 4, 'life': 5, 'this': 1, 'day': 6}\n"
     ]
    }
   ],
   "source": [
    "# Func: 按照出现频率大小, 记录下每一个单词的位置\n",
    "def position_lookup(v):\n",
    "    res = {}\n",
    "    counter = 0\n",
    "    for word in v:\n",
    "        res[word[0]] = counter\n",
    "        counter += 1\n",
    "    return res\n",
    "# 把标准的单词位置记录下来\n",
    "standard_position_dict = position_lookup(standard_freq_vector)\n",
    "print(standard_position_dict)\n",
    "# 得到一个位置对照表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'cool']\n",
      "[1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 这时, 如果我们有个新句子:\n",
    "sentence = 'this is cool'\n",
    "# 先新建一个跟我们的标准vector同样大小的向量量\n",
    "freq_vector = [0] * size\n",
    "# 简单的Preprocessing\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)\n",
    "# 对于这个新句子里里的每一个单词\n",
    "for word in tokens:\n",
    "    try:\n",
    "        # 如果在我们的词库里出现过\n",
    "        # 那么就在\"标准位置\"上+1\n",
    "        freq_vector[standard_position_dict[word]] += 1\n",
    "    except KeyError:\n",
    "        # 如果是个新词\n",
    "        # 就pass掉\n",
    "        continue\n",
    "print(freq_vector)\n",
    "# [1, 1, 0, 0, 0, 0, 0]\n",
    "# 第一个位置代表 is, 出现了了一次\n",
    "# 第二个位置代表 this, 出现了了一次\n",
    "# 后面都木有"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后可以当作特征扔进ML算法里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
