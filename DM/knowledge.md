## 一、数学
### 1.极大似然推导逻辑回归
[CSDN-逻辑回归公式推导过程](https://blog.csdn.net/weixin_30014549/article/details/52850870)


### 2.概率论知识
* 先验概率: 根据根据一般的经验、若干年的统计或者气候（常识），某地方下雨的概率；可理解为统计概率。 
* 后验概率: 考虑了一个事实之后的条件概率；根据天上有乌云（果），得到的下雨（因）的概率，即给定结果估计原因的概率；可理解为条件概率。
* 似然估计：在下雨（因）的情况下，观测到了乌云（果）的概率，即原因已知时，结果出现的概率；已知训练数据，给定了模型，通过让似然性极大化估计模型参数的一种方法

后验分布往往是基于先验分布和极大似然估计计算出来的。

* 条件概率公式：P(B|A) = P(A|B)*P(B) / P(A)  【P(A|B) = P(AB)/ P(B)】


### 3.熵
* 条件熵：定义为X给定条件下，Y的条件概率分布的熵对X的数学期望.    公式： H(X) = -求和p(xi)logp(xi)



## 二、机器学习
### 1.L1、L2正则
[CSDN-机器学习中正则化项L1和L2的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975)

### 2.生成模型&判别模型
都属于有监督。

* 生成模型的求解思路是：联合分布------->求解类别先验概率和类别条件概率；常见的生成方法有混合高斯模型、朴素贝叶斯法和隐形马尔科夫模型等，

* 判别模型求解的思路是：条件分布------>模型参数后验概率最大------->（似然函数 参数先验）最大------->最大似然。常见的判别方法有SVM、LR、NN、CRF。

##### 优缺点：

* 生成模型：

优点：
	1. 生成给出的是联合分布，不仅能够由联合分布计算条件分布（反之则不行），还可以给出其他信息，比如可以使用来计算边缘分布。如果一个输入样本的边缘分布很小的话，那么可以认为学习出的这个模型可能不太适合对这个样本进行分类，分类效果可能会不好，这也是所谓的outlier detection。
	2. 生成模型收敛速度比较快，即当样本数量较多时，生成模型能更快地收敛于真实模型。
	3. 生成模型能够应付存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。

缺点：1. 天下没有免费午餐，联合分布是能提供更多的信息，但也需要更多的样本和更多计算，尤其是为了更准确估计类别条件分布，需要增加样本的数目，而且类别条件概率的许多信息是我们做分类用不到，因而如果我们只需要做分类任务，就浪费了计算资源。2）另外，实践中多数情况下判别模型效果更好。

* 判别模型：优点：1）与生成模型缺点对应，首先是节省计算资源，另外，需要的样本数量也少于生成模型。2）准确率往往较生成模型高。3）由于直接学习，而不需要求解类别条件概率，所以允许我们对输入进行抽象（比如降维、构造等），从而能够简化学习问题。缺点：1）是没有生成模型的上述优点。

### 3.SVM推导&核函数
SVM目标是结构风险最小化。

* 推导：[博客](http://fire15.com/svm)
* 核函数: 低维映射到高维使线性可分。例如[x1,x2] -> [z1,z2,z3](z1=x1^2,z2=x2^2,z3=x2)。常用核函数有多项式核函数，高斯核函数等。

### 4.SVD PCA
* SVD: 不同于特征值分解，SVD（奇异值分解）可以作用于任何形状的矩阵。
* PCA: 一种对PCA的核心意图的解释是，找到另一组正交基P，使得X进行变换后的方差（variance）最大（因为选取了方差最大的维度，所以这样可以存储最多的信息）。

SVD是PCA的另一种algebraic formulation。而这也提供了另外一种算法来计算PCA，实际上，平时我就是用SVD定义的这套算法来做PCA的。因为很方便，计算一次就可以了。


### 5.HMM
* 五个基本要素：HMM是个五元组λ   ＝（ S, O , π ，A，B） S:状态值集合，O：观察值集合，π：初始化概率，A：状态转移概率矩阵，B：给定状态

* 三个假设
	1. 有限历史性假设，p(si|si-1,si-2,...,s1) = p(si|si-1)
	2. 齐次性假设，（状态与具体时间无关），P(si+1|si)=p(sj+1,sj)
	3. 输出独立性假设，输出仅与当前状态有关，P(o1,...ot|s1,...st) = P(ot|st)

* 三个问题
	1. 评估问题，已知模型参数 λ= (A, B, π),计算某个观测序列发生的概率，即求P(O|λ)用前向算法或者后向算法求解此类问题，以前向算法为例,前向算法给出了一个重要的变量---前向变量αt(i)表示在t时刻HMM输出给定观测序列O1O2...Ot，并且状态为Si的概率.
	2. 解码问题，给出观测序列O和模型μ，怎样选择一个状态序列S(s1,s2,...st+1),能最好的解释观测序列O此问题采用了维特比算法.
	3. 学习问题，如何调整模型参数 λ=(π, A, B), 使得P(O|λ)较大？此问题面向不同的语料库有不同的方式a.对于观测序列和状态序列都有的立项预料库，直接用较大似然估计即可获得参数λ b.对于只知道观测序列的，应用EM算法(Baum-Welch算法)的实现前向后向算法求解.

Baum-Welch算法就是从EM算法演变而来，又被称为前向后向算法。[CSDN-从EM算法到Baum-Welch算法](https://blog.csdn.net/firparks/article/details/54934112)


### 6.CRF
HMM模型是对转移概率和表现概率直接建模，统计共现概率。而MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率。CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。MEMM容易陷入局部最优，是因为MEMM只在局部做归一化。CRF模型中，统计了全局概率，在做归一化时，考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。
    CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。


### 7.线性分类器
线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。

* 感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。

* 支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）

* Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。
根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。

### 8.模型融合
* blending：
* stacking：比blending要好。[代码](https://github.com/fire717/Machine-Learning/blob/master/mine/Stacking.py) [使用示例](https://github.com/fire717/Machine-Learning/blob/master/mine/tryStacking.ipynb)
* weighted：加权求和、平均。
* bagging: 例如随即森林。用训练集的不同子集训练每个base model，最后进行每个base model权重相同的投票。
* boosting: 例如adaboost，xgboost，gbdt。迭代训练base model，根据上一次迭代错误的情况修改样本的权重，比bagging效果好，但容易过拟合。









